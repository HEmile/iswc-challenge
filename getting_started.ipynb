{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Large-scale language models (LMs) such as BERT are optimized to predict masked-out textual inputs and have notably advanced performances on a range of downstream NLP tasks. Recently, LMs also gained attention for their purported ability to yield structured pieces of knowledge directly from their parameters. This is promising as current knowledge bases (KBs) such as Wikidata and ConceptNet are part of the backbone of the Semantic Web ecosystem, yet are inherently incomplete. In the recent seminal LAMA paper, authors showed that LMs could highly rank correct object tokens when given an input prompt specifying the subject-entity and relation. Despite much follow-up work reporting further advancements, the prospect of using LMs for knowledge base construction remains unexplored. \n",
    "\n",
    "We invite participants to present solutions to make use of **LMs for KB construction** without prior information on the cardinality of relations, i.e., for a given subject-relation pair, the details on the total count of possible object-entities are absent. We require participants to submit a system that takes an input consisting of a subject-entity and relation, uses an LM depending on the choice of the track (BERT-type or open), generates subject-relation-object tuples, and makes actual accept/reject decisions for each generated output triple. Finally, we evaluate the resulting KBs using established F1-score (harmonic mean of precision and recall) metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Before continuing further, follow the steps given in README.md to install the required python packages, and download the dataset and supporting python scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Base Construction from Language Models (LM-KBC) pipeline has the following important modules:\n",
    "\n",
    "1. Choosing the subject-entity (e.g., Germany) and relation (e.g., CountryBordersWithCountry)\n",
    "2. Creating a prompt ( e.g., \"_Germany shares border with [MASK]_.\", a masked prompt for BERT-type masked language models)\n",
    "3. Probing an existing language model using the above prompt as an input\n",
    "4. Obtaining LM's output, which are the likelihood based ranked object-entities in the [MASK] position, using the  on the input prompt\n",
    "5. Applying a selection criteria on LM's output to get only the factually correct object-entitites for the given subject-entity and relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Participants can propose solutions that either improves the performance of these modules compared to the given baseline system or submit a new idea to better generate the object-entities, with the goal to beat the baseline F1-score of 14.21% on the hidden test dataset. Below we explain how some of these modules affect the LM's output when probed.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "from IPython.display import display\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m### importing all the functions from the baseline.py and evaluate.py scripts\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbaseline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mevaluate\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/Projects/iswc-hackathon/baseline.py:6\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForMaskedLM, AutoTokenizer, pipeline\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m### using GPU if available\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "### importing all the functions from the baseline.py and evaluate.py scripts\n",
    "\n",
    "from baseline import *\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assume the following subject entity and relation from here on\n",
    "subject_entity = 'Singapore'\n",
    "relation = 'CountryBordersWithCountry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of Languge Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the output object-entities varies for three different pre-trained LMs - BERT-base, BERT-large and RoBERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### probing the three different LMs on the chosen subject-entity and relation\n",
    "probe_lm('bert-base-cased', 100, relation, [subject_entity], Path('./prompt_output_bert_base/'))\n",
    "probe_lm('bert-large-cased', 100, relation, [subject_entity], Path('./prompt_output_bert_large/'))\n",
    "probe_lm('roberta-base', 100, relation, [subject_entity], Path('./prompt_output_roberta_base/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "setting the probability threshold equal to 0.5 (our selection criteria) and then running the baseline function \n",
    "on all the three models \n",
    "'''\n",
    "prob_threshold = 0.5\n",
    "baseline(Path('./prompt_output_bert_base/'), prob_threshold, [relation], Path('./bert_base_output/'))\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold, [relation], Path('./bert_large_output/'))\n",
    "baseline(Path('./prompt_output_roberta_base/'), prob_threshold,[relation], Path('./roberta_base_output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retriving the ground truth labels from the given train dataset for the chosen subject-entity and relation\n",
    "df = pd.read_csv('./train/'+relation+'.csv')\n",
    "df['ObjectEntity'] = df['ObjectEntity'].apply(literal_eval)\n",
    "df = df[df['SubjectEntity']==subject_entity]\n",
    "ground_truth = df['ObjectEntity'].tolist()\n",
    "print ('Ground truth object-entities are: ', ground_truth)\n",
    "\n",
    "### retriving the outputs obtained after running the baseline function\n",
    "bert_base_output = pd.read_csv('./bert_base_output/'+relation+'.csv')['ObjectEntity'].tolist()\n",
    "bert_large_output = pd.read_csv('./bert_large_output/'+relation+'.csv')['ObjectEntity'].tolist()\n",
    "roberta_base_output = pd.read_csv('./roberta_base_output/'+relation+'.csv')['ObjectEntity'].tolist()\n",
    "print ('bert_base_output: ', bert_base_output)\n",
    "print ('bert_large_output: ', bert_large_output)\n",
    "print ('roberta_base_output: ', roberta_base_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that the choice of the pre-trained language model has a direct effect on the generated output. Participants can try to further fine-tune the BERT model (for track 1) on this task or experiment with other existing pre-training LMs (for track 2).<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of prompt formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the output object-entities varies while using different prompt structures on BERT-large LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function\n",
    "def get_results(probe_outputs, prompt):\n",
    "    results = []\n",
    "    for sequence in probe_outputs:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Prompt\": prompt,\n",
    "                    \"SubjectEntity\": subject_entity,\n",
    "                    \"Relation\": relation,\n",
    "                    \"ObjectEntity\": sequence[\"token_str\"],\n",
    "                    \"Probability\": round(sequence[\"score\"], 4),\n",
    "                }\n",
    "            )\n",
    "    ### saving the prompt outputs separately for each relation type\n",
    "    results_df = pd.DataFrame(results).sort_values(\n",
    "        by=[\"SubjectEntity\", \"Probability\"], ascending=(True, False)\n",
    "    )\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initializing the bert-large model\n",
    "bert_large, bert_large_masked_token = initialize_lm('bert-large-cased', 100)\n",
    "\n",
    "### getting the sample prompt defined in the baseline.py script ({subject_entity} shares border with [MASK].)\n",
    "sample_prompt = create_prompt(subject_entity, relation, bert_large_masked_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating different prompts:\n",
    "\n",
    "### 1. ###({subject_entity} borders [MASK].) \n",
    "prompt1 = subject_entity + \" borders {}\".format(bert_large_masked_token) \n",
    "\n",
    "### 2. ({subject_entity} borders [MASK], which is a country.)\n",
    "prompt2 = subject_entity + \" borders {}, which is a country\".format(bert_large_masked_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### probing the BERT-large LM using the three different prompts for same subject-entity and relation\n",
    "sample_prompt_output = bert_large(sample_prompt)\n",
    "prompt1_output = bert_large(prompt1)\n",
    "prompt2_output = bert_large(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### storing the received output in a pandas dataframe\n",
    "sample_prompt_results = get_results(sample_prompt_output, sample_prompt)\n",
    "prompt1_results = get_results(prompt1_output, prompt1)\n",
    "prompt2_results = get_results(prompt2_output, prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [sample_prompt_results.head(3), prompt1_results.head(3), prompt2_results.head(3)]:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that the prompt used for probing affects the quality of the generated output. Participants can propose a solution that automatically designs better and optimal prompts for this task.<font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of selection criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the choosing different the probability thresholds affects the generated output object-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### probing the BERT-large model on the chosen subject-entity and relation\n",
    "probe_lm('bert-large-cased', 100, relation, [subject_entity], Path('./prompt_output_bert_large/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initializing different probability thresholds\n",
    "prob_threshold1 = 0.1\n",
    "prob_threshold2 = 0.5\n",
    "prob_threshold3 = 0.9\n",
    "\n",
    "### running the baseline function on the above three thresholds\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold1, [relation], Path('./thres1_output/'))\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold2, [relation], Path('./thres2_output/'))\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold3,[relation], Path('./thres3_output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres1_result = pd.read_csv('./thres1_output/'+relation+'.csv')\n",
    "thres2_result = pd.read_csv('./thres2_output/'+relation+'.csv')\n",
    "thres3_result = pd.read_csv('./thres3_output/'+relation+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [thres1_result.head(3), thres2_result.head(3), thres3_result.head(3)]:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that changing the threshold leads to very different performance scores. When the threshold is 0.1, F1-score would be 0.01 (1 out of 2 generations is correct and 1 out of the two ground truth object-entities was selected); however for threshold 0.9, F1-score would be 0. Participants can propose a solution that uses a better thresholding mechanism or even further calibrate the LM's likelihood on this task.<font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}