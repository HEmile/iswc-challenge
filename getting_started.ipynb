{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Large-scale language models (LMs) such as BERT are optimized to predict masked-out textual inputs and have notably advanced performances on a range of downstream NLP tasks. Recently, LMs also gained attention for their purported ability to yield structured pieces of knowledge directly from their parameters. This is promising as current knowledge bases (KBs) such as Wikidata and ConceptNet are part of the backbone of the Semantic Web ecosystem, yet are inherently incomplete. In the recent seminal LAMA paper, authors showed that LMs could highly rank correct object tokens when given an input prompt specifying the subject-entity and relation. Despite much follow-up work reporting further advancements, the prospect of using LMs for knowledge base construction remains unexplored. \n",
    "\n",
    "We invite participants to present solutions to make use of **LMs for KB construction** without prior information on the cardinality of relations, i.e., for a given subject-relation pair, the details on the total count of possible object-entities are absent. We require participants to submit a system that takes an input consisting of a subject-entity and relation, uses an LM depending on the choice of the track (BERT-type or open), generates subject-relation-object tuples, and makes actual accept/reject decisions for each generated output triple. Finally, we evaluate the resulting KBs using established F1-score (harmonic mean of precision and recall) metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**NOTE:** Before continuing further, follow the steps given in README.md to install the required python packages, and download the dataset and supporting python scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LM Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Knowledge Base Construction from Language Models (LM-KBC) pipeline has the following important modules:\n",
    "\n",
    "1. Choosing the subject-entity (e.g., Germany) and relation (e.g., CountryBordersWithCountry)\n",
    "2. Creating a prompt ( e.g., \"_Germany shares border with [MASK]_.\", a masked prompt for BERT-type masked language models)\n",
    "3. Probing an existing language model using the above prompt as an input\n",
    "4. Obtaining LM's output, which are the likelihood based ranked object-entities in the [MASK] position, using the  on the input prompt\n",
    "5. Applying a selection criteria on LM's output to get only the factually correct object-entitites for the given subject-entity and relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>Participants can propose solutions that either improves the performance of these modules compared to the given baseline system or submit a new idea to better generate the object-entities, with the goal to beat the baseline F1-score of 14.21% on the hidden test dataset. Below we explain how some of these modules affect the LM's output when probed.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "from IPython.display import display\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### importing all the functions from the baseline.py and evaluate.py scripts\n",
    "\n",
    "from baseline import *\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Assume the following subject entity and relation from here on\n",
    "subject_entity = 'Singapore'\n",
    "relation = 'CountryBordersWithCountry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Effect of Languge Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how the output object-entities varies for three different pre-trained LMs - BERT-base, BERT-large and RoBERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probing the bert-base-cased language model for Singapore (subject-entity) and CountryBordersWithCountry relation\n",
      "Probing the bert-large-cased language model for Singapore (subject-entity) and CountryBordersWithCountry relation\n",
      "Probing the roberta-base language model for Singapore (subject-entity) and CountryBordersWithCountry relation\n"
     ]
    }
   ],
   "source": [
    "### probing the three different LMs on the chosen subject-entity and relation\n",
    "probe_lm('bert-base-cased', 100, relation, [subject_entity], Path('./prompt_output_bert_base/'))\n",
    "probe_lm('bert-large-cased', 100, relation, [subject_entity], Path('./prompt_output_bert_large/'))\n",
    "probe_lm('roberta-base', 100, relation, [subject_entity], Path('./prompt_output_roberta_base/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the baseline method ...\n",
      "Running the baseline method ...\n",
      "Running the baseline method ...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "setting the probability threshold equal to 0.5 (our selection criteria) and then running the baseline function \n",
    "on all the three models \n",
    "'''\n",
    "prob_threshold = 0.5\n",
    "baseline(Path('./prompt_output_bert_base/'), prob_threshold, [relation], Path('./bert_base_output/'))\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold, [relation], Path('./bert_large_output/'))\n",
    "baseline(Path('./prompt_output_roberta_base/'), prob_threshold,[relation], Path('./roberta_base_output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth object-entities are:  [['malaysia'], ['indonesia']]\n",
      "bert_base_output:  []\n",
      "bert_large_output:  ['Malaysia']\n",
      "roberta_base_output:  []\n"
     ]
    }
   ],
   "source": [
    "### retriving the ground truth labels from the given train dataset for the chosen subject-entity and relation\n",
    "df = pd.read_csv('./train/'+relation+'.csv')\n",
    "df['ObjectEntity'] = df['ObjectEntity'].apply(literal_eval)\n",
    "df = df[df['SubjectEntity']==subject_entity]\n",
    "ground_truth = df['ObjectEntity'].tolist()\n",
    "print ('Ground truth object-entities are: ', ground_truth)\n",
    "\n",
    "### retriving the outputs obtained after running the baseline function\n",
    "bert_base_output = pd.read_csv('./bert_base_output/'+relation+'.csv')['ObjectEntity'].tolist()\n",
    "bert_large_output = pd.read_csv('./bert_large_output/'+relation+'.csv')['ObjectEntity'].tolist()\n",
    "roberta_base_output = pd.read_csv('./roberta_base_output/'+relation+'.csv')['ObjectEntity'].tolist()\n",
    "print ('bert_base_output: ', bert_base_output)\n",
    "print ('bert_large_output: ', bert_large_output)\n",
    "print ('roberta_base_output: ', roberta_base_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that the choice of the pre-trained language model has a direct effect on the generated output. Participants can try to further fine-tune the BERT model (for track 1) on this task or experiment with other existing pre-training LMs (for track 2).<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Effect of prompt formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how the output object-entities varies while using different prompt structures on BERT-large LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### helper function\n",
    "def get_results(probe_outputs, prompt):\n",
    "    results = []\n",
    "    for sequence in probe_outputs:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Prompt\": prompt,\n",
    "                    \"SubjectEntity\": subject_entity,\n",
    "                    \"Relation\": relation,\n",
    "                    \"ObjectEntity\": sequence[\"token_str\"],\n",
    "                    \"Probability\": round(sequence[\"score\"], 4),\n",
    "                }\n",
    "            )\n",
    "    ### saving the prompt outputs separately for each relation type\n",
    "    results_df = pd.DataFrame(results).sort_values(\n",
    "        by=[\"SubjectEntity\", \"Probability\"], ascending=(True, False)\n",
    "    )\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### initializing the bert-large model\n",
    "bert_large, bert_large_masked_token = initialize_lm('bert-large-cased', 100)\n",
    "\n",
    "### getting the sample prompt defined in the baseline.py script ({subject_entity} shares border with [MASK].)\n",
    "sample_prompt = create_prompt(subject_entity, relation, bert_large_masked_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### creating different prompts:\n",
    "\n",
    "### 1. ###({subject_entity} borders [MASK].) \n",
    "prompt1 = subject_entity + \" borders {}\".format(bert_large_masked_token) \n",
    "\n",
    "### 2. ({subject_entity} borders [MASK], which is a country.)\n",
    "prompt2 = subject_entity + \" borders {}, which is a country\".format(bert_large_masked_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### probing the BERT-large LM using the three different prompts for same subject-entity and relation\n",
    "sample_prompt_output = bert_large(sample_prompt)\n",
    "prompt1_output = bert_large(prompt1)\n",
    "prompt2_output = bert_large(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### storing the received output in a pandas dataframe\n",
    "sample_prompt_results = get_results(sample_prompt_output, sample_prompt)\n",
    "prompt1_results = get_results(prompt1_output, prompt1)\n",
    "prompt2_results = get_results(prompt2_output, prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                 Prompt SubjectEntity  \\\n0  Singapore shares border with [MASK].     Singapore   \n1  Singapore shares border with [MASK].     Singapore   \n2  Singapore shares border with [MASK].     Singapore   \n\n                    Relation ObjectEntity  Probability  \n0  CountryBordersWithCountry     Malaysia       0.6908  \n1  CountryBordersWithCountry     Thailand       0.1125  \n2  CountryBordersWithCountry    Indonesia       0.0671  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>SubjectEntity</th>\n      <th>Relation</th>\n      <th>ObjectEntity</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Singapore shares border with [MASK].</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Malaysia</td>\n      <td>0.6908</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Singapore shares border with [MASK].</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Thailand</td>\n      <td>0.1125</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Singapore shares border with [MASK].</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Indonesia</td>\n      <td>0.0671</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                     Prompt SubjectEntity                   Relation  \\\n0  Singapore borders [MASK]     Singapore  CountryBordersWithCountry   \n1  Singapore borders [MASK]     Singapore  CountryBordersWithCountry   \n2  Singapore borders [MASK]     Singapore  CountryBordersWithCountry   \n\n  ObjectEntity  Probability  \n0            ;       0.5179  \n1            .       0.4814  \n2            |       0.0004  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>SubjectEntity</th>\n      <th>Relation</th>\n      <th>ObjectEntity</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Singapore borders [MASK]</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>;</td>\n      <td>0.5179</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Singapore borders [MASK]</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>.</td>\n      <td>0.4814</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Singapore borders [MASK]</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>|</td>\n      <td>0.0004</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                         Prompt SubjectEntity  \\\n0  Singapore borders [MASK], which is a country     Singapore   \n1  Singapore borders [MASK], which is a country     Singapore   \n2  Singapore borders [MASK], which is a country     Singapore   \n\n                    Relation ObjectEntity  Probability  \n0  CountryBordersWithCountry     Malaysia       0.3827  \n1  CountryBordersWithCountry    Indonesia       0.1468  \n2  CountryBordersWithCountry     Thailand       0.0947  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>SubjectEntity</th>\n      <th>Relation</th>\n      <th>ObjectEntity</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Singapore borders [MASK], which is a country</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Malaysia</td>\n      <td>0.3827</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Singapore borders [MASK], which is a country</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Indonesia</td>\n      <td>0.1468</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Singapore borders [MASK], which is a country</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Thailand</td>\n      <td>0.0947</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [sample_prompt_results.head(3), prompt1_results.head(3), prompt2_results.head(3)]:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that the prompt used for probing affects the quality of the generated output. Participants can propose a solution that automatically designs better and optimal prompts for this task.<font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Effect of selection criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how the choosing different the probability thresholds affects the generated output object-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probing the bert-large-cased language model for Singapore (subject-entity) and CountryBordersWithCountry relation\n"
     ]
    }
   ],
   "source": [
    "### probing the BERT-large model on the chosen subject-entity and relation\n",
    "probe_lm('bert-large-cased', 100, relation, [subject_entity], Path('./prompt_output_bert_large/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the baseline method ...\n",
      "Running the baseline method ...\n",
      "Running the baseline method ...\n"
     ]
    }
   ],
   "source": [
    "### initializing different probability thresholds\n",
    "prob_threshold1 = 0.1\n",
    "prob_threshold2 = 0.5\n",
    "prob_threshold3 = 0.9\n",
    "\n",
    "### running the baseline function on the above three thresholds\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold1, [relation], Path('./thres1_output/'))\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold2, [relation], Path('./thres2_output/'))\n",
    "baseline(Path('./prompt_output_bert_large/'), prob_threshold3,[relation], Path('./thres3_output/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "thres1_result = pd.read_csv('./thres1_output/'+relation+'.csv')\n",
    "thres2_result = pd.read_csv('./thres2_output/'+relation+'.csv')\n",
    "thres3_result = pd.read_csv('./thres3_output/'+relation+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                 Prompt SubjectEntity  \\\n0  Singapore shares border with [MASK].     Singapore   \n1  Singapore shares border with [MASK].     Singapore   \n\n                    Relation ObjectEntity  Probability  \n0  CountryBordersWithCountry     Malaysia       0.6908  \n1  CountryBordersWithCountry     Thailand       0.1125  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>SubjectEntity</th>\n      <th>Relation</th>\n      <th>ObjectEntity</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Singapore shares border with [MASK].</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Malaysia</td>\n      <td>0.6908</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Singapore shares border with [MASK].</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Thailand</td>\n      <td>0.1125</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                 Prompt SubjectEntity  \\\n0  Singapore shares border with [MASK].     Singapore   \n\n                    Relation ObjectEntity  Probability  \n0  CountryBordersWithCountry     Malaysia       0.6908  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>SubjectEntity</th>\n      <th>Relation</th>\n      <th>ObjectEntity</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Singapore shares border with [MASK].</td>\n      <td>Singapore</td>\n      <td>CountryBordersWithCountry</td>\n      <td>Malaysia</td>\n      <td>0.6908</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [Prompt, SubjectEntity, Relation, ObjectEntity, Probability]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>SubjectEntity</th>\n      <th>Relation</th>\n      <th>ObjectEntity</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [thres1_result.head(3), thres2_result.head(3), thres3_result.head(3)]:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color='blue'>**Observation**: From the above output, we see that changing the threshold leads to very different performance scores. When the threshold is 0.1, F1-score would be 0.01 (1 out of 2 generations is correct and 1 out of the two ground truth object-entities was selected); however for threshold 0.9, F1-score would be 0. Participants can propose a solution that uses a better thresholding mechanism or even further calibrate the LM's likelihood on this task.<font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}